{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Доброго времени суток. На этой неделе защищаю диплом, поэтому не получилось уделить достаточно времени для изучения темы. Сделала ЛР по имеющимся знаниям и опыту. Не уверена в правильности."
      ],
      "metadata": {
        "id": "bwsxHh4OMa8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Attention\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "Pp6ms-9zDvol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузка данных"
      ],
      "metadata": {
        "id": "pjJcBL3AEIzu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "№Была использована нейронная сеть Seq2Seq с механизмом Attention для обучения переводчика с русского на английский язык, используя доступные данные."
      ],
      "metadata": {
        "id": "B30WZMcOpJ1Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1BaSz95A6r7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65aa091f-3cf4-46eb-b091-e57087e05eaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    243 Один раз в жизни я делаю хорошее дело... И оно бесполезно.     3257  \\\n",
            "0  5409                      Давайте что-нибудь попробуем!             1276   \n",
            "1  5410                               Мне пора идти спать.             1277   \n",
            "2  5411                                    Что ты делаешь?            16492   \n",
            "3  5411                                    Что ты делаешь?           511884   \n",
            "4  5411                                    Что ты делаешь?          1839618   \n",
            "\n",
            "  For once in my life I'm doing a good deed... And it is useless.  \n",
            "0                               Let's try something.               \n",
            "1                             I have to go to sleep.               \n",
            "2                                What are you doing?               \n",
            "3                                  What do you make?               \n",
            "4                                 What're you doing?               \n"
          ]
        }
      ],
      "source": [
        "file_path = 'Rus-Eng-small.tsv'\n",
        "\n",
        "# Чтение файла в DataFrame\n",
        "df = pd.read_csv(file_path, delimiter='\\t')\n",
        "\n",
        "# Отображение первых нескольких строк DataFrame\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Предобработка данных\n",
        "def preprocess_sentences(sentences):\n",
        "    cleaned_sentences = []\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.lower().strip()\n",
        "        sentence = '<start> ' + sentence + ' <end>'\n",
        "        cleaned_sentences.append(sentence)\n",
        "    return cleaned_sentences\n",
        "\n",
        "# Извлечение данных\n",
        "sample_size = 25000\n",
        "df_sample = df.sample(n=sample_size, random_state=42)\n",
        "\n",
        "rus_sentences = df_sample.iloc[:, 1].astype(str).values\n",
        "eng_sentences = df_sample.iloc[:, 2].astype(str).values\n",
        "\n",
        "# Очистка и добавление токенов\n",
        "rus_sentences = preprocess_sentences(rus_sentences)\n",
        "eng_sentences = preprocess_sentences(eng_sentences)\n",
        "\n",
        "# Токенизация\n",
        "rus_tokenizer = Tokenizer()\n",
        "eng_tokenizer = Tokenizer()\n",
        "\n",
        "rus_tokenizer.fit_on_texts(rus_sentences)\n",
        "eng_tokenizer.fit_on_texts(eng_sentences)\n",
        "\n",
        "rus_sequences = rus_tokenizer.texts_to_sequences(rus_sentences)\n",
        "eng_sequences = eng_tokenizer.texts_to_sequences(eng_sentences)\n",
        "\n",
        "max_rus_len = max(len(seq) for seq in rus_sequences)\n",
        "max_eng_len = max(len(seq) for seq in eng_sequences)\n",
        "\n",
        "rus_sequences = pad_sequences(rus_sequences, maxlen=max_rus_len, padding='post')\n",
        "eng_sequences = pad_sequences(eng_sequences, maxlen=max_eng_len, padding='post')\n",
        "\n",
        "rus_vocab_size = len(rus_tokenizer.word_index) + 1\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "\n",
        "print(f\"Форма русских последовательностей: {rus_sequences.shape}\")\n",
        "print(f\"Форма английских последовательностей: {eng_sequences.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRIF235bHxU2",
        "outputId": "ab5445e6-5464-40fb-9f95-514fc0beecf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Форма русских последовательностей: (25000, 115)\n",
            "Форма английских последовательностей: (25000, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, Attention\n",
        "import tensorflow as tf\n",
        "\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "\n",
        "# Энкодер\n",
        "encoder_inputs = Input(shape=(max_rus_len,))\n",
        "encoder_embedding = Embedding(input_dim=rus_vocab_size, output_dim=embedding_dim)(encoder_inputs)\n",
        "encoder_lstm = LSTM(units, return_sequences=True, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "\n",
        "# Декодер\n",
        "decoder_inputs = Input(shape=(max_eng_len - 1,))\n",
        "decoder_embedding = Embedding(input_dim=eng_vocab_size, output_dim=embedding_dim)(decoder_inputs)\n",
        "decoder_lstm = LSTM(units, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])\n",
        "\n",
        "# Механизм внимания\n",
        "attention = Attention()\n",
        "context_vector = attention([decoder_outputs, encoder_outputs])\n",
        "decoder_combined_context = Concatenate(axis=-1)([context_vector, decoder_outputs])\n",
        "\n",
        "# Полносвязный слой\n",
        "decoder_dense = Dense(eng_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_combined_context)\n",
        "\n",
        "# Модель\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Компиляция модели\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "decoder_input_data = eng_sequences[:, :-1]\n",
        "decoder_target_data = eng_sequences[:, 1:]\n",
        "\n",
        "# Обучение модели\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "\n",
        "history = model.fit(\n",
        "    [rus_sequences, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.2\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1kbbyEVH4tK",
        "outputId": "70b5be43-26bb-4c94-cde4-570831d6a8cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "313/313 [==============================] - 1214s 4s/step - loss: 5.4812 - accuracy: 0.4887 - val_loss: 5.2816 - val_accuracy: 0.5000\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - 1193s 4s/step - loss: 5.0967 - accuracy: 0.4999 - val_loss: 5.6676 - val_accuracy: 0.5000\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - 1191s 4s/step - loss: 4.9875 - accuracy: 0.5001 - val_loss: 5.9047 - val_accuracy: 0.5000\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - 1176s 4s/step - loss: 4.8788 - accuracy: 0.5000 - val_loss: 6.2152 - val_accuracy: 0.5000\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - 1180s 4s/step - loss: 4.6399 - accuracy: 0.5003 - val_loss: 6.4104 - val_accuracy: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Декодирование последовательностей и тестирование модел"
      ],
      "metadata": {
        "id": "udXK1rY7rLci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание модели энкодера для декодирования\n",
        "encoder_model = Model(encoder_inputs, [encoder_outputs, state_h, state_c])\n",
        "\n",
        "# Создание модели декодера для декодирования\n",
        "decoder_state_input_h = Input(shape=(units,))\n",
        "decoder_state_input_c = Input(shape=(units,))\n",
        "decoder_hidden_state_input = Input(shape=(max_rus_len, units))\n",
        "\n",
        "decoder_embedding2 = Embedding(input_dim=eng_vocab_size, output_dim=embedding_dim)\n",
        "decoder_inputs2 = Input(shape=(None,))\n",
        "decoder_embedding_outputs = decoder_embedding2(decoder_inputs2)\n",
        "\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(\n",
        "    decoder_embedding_outputs, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "context_vector2 = attention([decoder_outputs2, decoder_hidden_state_input])\n",
        "decoder_combined_context2 = Concatenate(axis=-1)([context_vector2, decoder_outputs2])\n",
        "decoder_outputs2 = decoder_dense(decoder_combined_context2)\n",
        "\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs2, decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs2, state_h2, state_c2])\n"
      ],
      "metadata": {
        "id": "H3qlO2hErIt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  def decode_sequence(input_seq):\n",
        "      encoder_outputs, h, c = encoder_model.predict(input_seq)\n",
        "\n",
        "      target_seq = np.zeros((1, 1))\n",
        "      target_seq[0, 0] = eng_tokenizer.word_index.get('<start>', 0)\n",
        "\n",
        "      stop_condition = False\n",
        "      decoded_sentence = ''\n",
        "      while not stop_condition:\n",
        "          output_tokens, h, c = decoder_model.predict(\n",
        "              [target_seq, encoder_outputs, h, c])\n",
        "\n",
        "          sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "          sampled_word = eng_tokenizer.index_word.get(sampled_token_index, '')\n",
        "\n",
        "          if sampled_word:\n",
        "              decoded_sentence += ' ' + sampled_word\n",
        "\n",
        "          if (sampled_word == '<end>' or len(decoded_sentence.split()) > max_eng_len):\n",
        "              stop_condition = True\n",
        "\n",
        "          target_seq = np.zeros((1, 1))\n",
        "          target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "      return decoded_sentence\n"
      ],
      "metadata": {
        "id": "Cw0wNEIpIGnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for seq_index in range(10):\n",
        "    input_seq = rus_sequences[seq_index:seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('Input:', rus_sentences[seq_index])\n",
        "    print('Decoded:', decoded_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXrIDwNFIIJn",
        "outputId": "a9c7d2a7-52a9-4397-9205-98741efd96ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 521ms/step\n",
            "1/1 [==============================] - 1s 609ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "Input: <start> том умнее, чем ты думаешь. <end>\n",
            "Decoded:  2375818 end end end\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "Input: <start> что вы собираетесь делать? <end>\n",
            "Decoded:  end end end end\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "Input: <start> я хочу поблагодарить тебя. <end>\n",
            "Decoded:  3161734 end end end\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Input: <start> это было ужасно. <end>\n",
            "Decoded:  2255462 end end end\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Input: <start> веди ты. <end>\n",
            "Decoded:  2255462 end end end\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Input: <start> чего я действительно не могу понять, так это зачем запрещать детям играть на улице в такой замечательный день. <end>\n",
            "Decoded:  1936442 end end end\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Input: <start> можем встретиться на следующей неделе, если хотите. <end>\n",
            "Decoded:  276976 end end end\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "Input: <start> старик освободил лисёнка из капкана. <end>\n",
            "Decoded:  end end end end\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Input: <start> как этот диван выглядит без чехла? <end>\n",
            "Decoded:  4015049 end end end\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Input: <start> я тихонько закрыл дверь, чтобы не потревожить малыша. <end>\n",
            "Decoded:  2033666 end end end\n"
          ]
        }
      ]
    }
  ]
}